{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from soynlp.noun import LRNounExtractor, LRNounExtractor_v2\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv('./youtube_channel_comments_data_20240606_104600.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56453 entries, 0 to 56452\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   channelId           56453 non-null  object\n",
      " 1   channelTitle        56453 non-null  object\n",
      " 2   channelPublishedAt  56453 non-null  object\n",
      " 3   subscriberCount     56453 non-null  int64 \n",
      " 4   videoId             56453 non-null  object\n",
      " 5   videoTitle          56453 non-null  object\n",
      " 6   videoAuthorId       56453 non-null  object\n",
      " 7   videoPublishedAt    56453 non-null  object\n",
      " 8   duration            56453 non-null  object\n",
      " 9   viewCount           56453 non-null  int64 \n",
      " 10  likeCount           56453 non-null  int64 \n",
      " 11  dislikeCount        56453 non-null  int64 \n",
      " 12  commentId           56453 non-null  object\n",
      " 13  commentAuthor       56325 non-null  object\n",
      " 14  authorId            56453 non-null  object\n",
      " 15  commentText         56449 non-null  object\n",
      " 16  commentLikeCount    56453 non-null  int64 \n",
      " 17  commentPublishedAt  56453 non-null  object\n",
      " 18  parentCommentId     27851 non-null  object\n",
      "dtypes: int64(5), object(14)\n",
      "memory usage: 8.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         야채 많이드시고 가끔 요구르트 챙겨드시면\\n어느정도 예방됨\n",
       "1                            제일 확실한건 변비 설사 반복 됩니다 꼭 검사해보셔요\n",
       "2                                           좋은정보 감사합니다 😊😊😊\n",
       "3                                        걸리면 하느님 만나로 올라갈게요\n",
       "4                                이중에 하나도 해당 안됐는데\\n대장암 4기..\n",
       "                               ...                        \n",
       "56448            이지약사님 정말 오랜만에 뵙네요 그동안 잘 지내셨나요\\n너무 보고 싶었어요\n",
       "56449                                     감사합니다 잘 지내셨죠~~!!\n",
       "56450    이지 약사님, 정말 오랜만에 뵙네요. 그동안 잘 지내셨나요?\\n너무 보고 싶었어요. ^^\n",
       "56451            안녕하세요 감사합니다..ㅎㅎ 부활해 보았습니당..ㅎㅎ 무더위 잘 보내셨죠!\n",
       "56452                 @@ezyaksa 이지 약사님을 이렇게 뵈니 너무 반갑네요. ㅎㅎ\n",
       "Name: commentText, Length: 56453, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaned_commentText의 데이터 보기\n",
    "data['commentText']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 문자 제거 함수\n",
    "def clean_text(text):\n",
    "    # 줄바꿈 문자 포함하여 특수 문자 제거\n",
    "    text = re.sub(r'[^ㄱ-ㅎ가-힣0-9\\s]', '', text)\n",
    "    # \"ㅎㅎ\", \"ㅋㅋ\" 등 비정식 단어 제거\n",
    "    text = re.sub(r'ㅎㅎ|ㅋㅋ', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # 공백 문자를 하나의 공백으로 치환하고 양쪽 공백 제거\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN 값 제거\n",
    "data = data.dropna(subset=['commentText'])\n",
    "\n",
    "# NaN 값 공백 문자열로 대체\n",
    "data['commentText'] = data['commentText'].fillna('')\n",
    "\n",
    "# 댓글, 답글 데이터 전처리\n",
    "data['cleaned_commentText'] = data['commentText'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  야채 많이드시고 가끔 요구르트 챙겨드시면 어느정도 예방됨\n",
       "1                    제일 확실한건 변비 설사 반복 됩니다 꼭 검사해보셔요\n",
       "2                                       좋은정보 감사합니다\n",
       "3                                걸리면 하느님 만나로 올라갈게요\n",
       "4                           이중에 하나도 해당 안됐는데 대장암 4기\n",
       "                           ...                    \n",
       "56448     이지약사님 정말 오랜만에 뵙네요 그동안 잘 지내셨나요 너무 보고 싶었어요\n",
       "56449                                 감사합니다 잘 지내셨죠\n",
       "56450    이지 약사님 정말 오랜만에 뵙네요 그동안 잘 지내셨나요 너무 보고 싶었어요\n",
       "56451             안녕하세요 감사합니다 부활해 보았습니당 무더위 잘 보내셨죠\n",
       "56452                       이지 약사님을 이렇게 뵈니 너무 반갑네요\n",
       "Name: cleaned_commentText, Length: 56449, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_commentText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.441 Gbory 0.227 Gb\n",
      "all cohesion probabilities was computed. # words = 46279\n",
      "all branching entropies was computed # words = 63382\n",
      "all accessor variety was computed # words = 63382\n"
     ]
    }
   ],
   "source": [
    "# soynlp 훈련\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(data['cleaned_commentText'])\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. soynlp 토크나이저 구축\n",
    "soynlp의 WordExtractor로부터 얻은 word_score_table을 이용하여 LTokenizer 또는 MaxScoreTokenizer를 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 145468 from 56449 sents. mem=0.453 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=665231, mem=0.682 Gb\n",
      "[Noun Extractor] batch prediction was completed for 43450 words\n",
      "[Noun Extractor] checked compounds. discovered 28041 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38152 -> 29037\n",
      "[Noun Extractor] postprocessing ignore_features : 29037 -> 28831\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28831 -> 28390\n",
      "[Noun Extractor] 28390 nouns (28041 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.754 Gb                    \n",
      "[Noun Extractor] 69.04 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "# 코헤전스 점수 계산\n",
    "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "# 명사 추출\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(data['cleaned_commentText']) # list of str like\n",
    "\n",
    "noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "combined_scores = {noun:score + scores.get(noun, 0)\n",
    "    for noun, score in noun_scores.items()}\n",
    "\n",
    "combined_scores.update(\n",
    "    {subword:cohesion for subword, cohesion in scores.items()\n",
    "    if not (subword in combined_scores)}\n",
    ")\n",
    "\n",
    "tokenizer = LTokenizer(scores=combined_scores)\n",
    "# maxscore_tokenizer = MaxScoreTokenizer(scores=combined_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 텍스트 토크나이징\n",
    "토크나이저를 사용하여 텍스트 데이터를 토큰화합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohension score인 scores dict 변수에 직접 추가\n",
    "combined_scores[\"네츄럴팩터스\"] = 1.0\n",
    "# scores[\"레게노\"] = 1.0\n",
    "# scores[\"돈쭐\"] = 1.0\n",
    "\n",
    "\n",
    "\n",
    "data['tokenized'] = data['cleaned_commentText'].apply(tokenizer.tokenize)\n",
    "# data['maxscore_tokenized'] = data['cleaned_commentText'].apply(maxscore_tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LTokenizer\n",
    "# 한글자, 지시대명사 등 제거\n",
    "data['tokenized'] = data['tokenized'].apply(lambda x: [item for item in x if len(item) > 1])\n",
    "\n",
    "# 한글자 이외의 형용사, 부사, 동사 불용어 stopword 목록 작성하여 반영\n",
    "stopword = ['ㅎㅎ', 'ㅋㅋ', 'ㅠㅠ', 'ㅜㅜ', 'ㅇㅇ', 'ㅈㅈ', 'ㅊㅊ', 'ㅋㅋ', 'ㅎㅎ', 'ㅇㅇ', 'ㅈㅈ', 'ㅊㅊ']\n",
    "data['tokenized'] = data['tokenized'].apply(lambda x: [item for item in x if item not in stopword])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0:\n",
      "L Tokenized:  ['야채', '많이', '드시고', '가끔', '요구르트', '챙겨', '드시면', '어느정도', '예방']\n",
      "Row 1:\n",
      "L Tokenized:  ['제일', '확실', '한건', '변비', '설사', '반복', '됩니', '검사', '해보셔요']\n",
      "Row 2:\n",
      "L Tokenized:  ['좋은정보', '감사', '합니다']\n",
      "Row 3:\n",
      "L Tokenized:  ['걸리', '하느님', '만나로', '올라', '갈게요']\n",
      "Row 4:\n",
      "L Tokenized:  ['이중', '하나', '해당', '안됐는데', '대장암', '4기']\n",
      "Row 5:\n",
      "L Tokenized:  ['힘내세요']\n",
      "Row 6:\n",
      "L Tokenized:  []\n",
      "Row 7:\n",
      "L Tokenized:  ['헉진짜', '힘내세요', '좋은일', '가득', '하시길바래요']\n",
      "Row 8:\n",
      "L Tokenized:  ['힘내세요']\n",
      "Row 9:\n",
      "L Tokenized:  ['체중감소', '에서', '안심']\n",
      "Row 10:\n",
      "L Tokenized:  ['질병', '없는', '사람', '먹어도', '되나요']\n",
      "Row 11:\n",
      "L Tokenized:  ['내츄럴', '팩터스', '베르베린', '먹고있는데', '휴지기', '가져야', '할까요']\n",
      "Row 12:\n",
      "L Tokenized:  ['제일', '마지막', '제품', '내츄럴팩터', '베르베린', '리포미셀', '구매', '했는데', '하루', '몇번', '몇알', '섭취', '하는', '건가요']\n",
      "Row 13:\n",
      "L Tokenized:  ['당뇨환자', '먹고', '있는데', '먹을', '있는', '영양제', '알려주세요']\n",
      "Row 14:\n",
      "L Tokenized:  ['샘요즘', '알코틴클렌저', '라고', '니코틴', '알코올', '분해', '해준다는', '영양제', '광고', '해서', '약국', '에서도', '판매', '한다고', '되어있는데', '효과가', '정말', '있는', '건가요']\n",
      "Row 15:\n",
      "L Tokenized:  ['당뇨', '병은', '먹고', '움직', '이는', '당뇨', '얼씬도', '하죠', '그리고', '아무리', '채소', '먹고', '움직', '여도', '간이', '좋으면', '결국', '당뇨', '걸리', '경우', '많이', '봤어요', '간이', '좋으려면', '영양제', '많이', '먹으면', '좋습니다', '지병', '있는데', '영양제', '유산균', '비타민', '비타민', '마그네슘', '엽산', '5가지', '먹는데', '간수치', '좋아요', '항상', '검사', '1112', '나와요', '간이', '좋아야', '당뇨', '옵니다']\n",
      "Row 16:\n",
      "L Tokenized:  ['오메가', '부작용', '으로', '시끄러운데', '한번', '다뤄주실수', '있을까요']\n",
      "Row 17:\n",
      "L Tokenized:  ['바나바잎', '베르베린', '함께', '섭취', '해도', '괜찮', '을까요', '저는', '당뇨', '환자', '인데', '6개월', '정도', '먹고', '있는데요', '혹시', '부딪치는', '증상', '생길까', '해서', '여쭙습니다']\n",
      "Row 18:\n",
      "L Tokenized:  ['약사님평소', '보고', '있습니다', '폐경이후', '먹어야', '약이', '영양제', '가있으면', '좀알려주세요', '에스트로겐섭취도해야된다', '얘기', '들이많던데', '저는', '잘모르겠네요']\n",
      "Row 19:\n",
      "L Tokenized:  ['폐경이후', '에도', '섭취', '하시는', '비슷', '합니다', '다만', '기본적', '오마비디유씨', '함께', '이소플라본', '미오이노시톨', '같은', '에스트로겐', '관련', '제품', '뼈건강', '관련', '칼슘', '마그네슘비타민2와', '철분', '부족', '해질', '있으니', '철분정도', '좋을', '합니다']\n",
      "Row 20:\n",
      "L Tokenized:  ['바쁘실텐데', '달아주셔서', '매우', '감사', '합니다']\n",
      "Row 21:\n",
      "L Tokenized:  ['의사', '상의', '구매', '하세요', '이렇게', '좋으면', '의사', '처방', '하지', '않을까']\n",
      "Row 22:\n",
      "L Tokenized:  ['확실', '하게', '섭취', '하시는', '약과', '증상', '치료', '에에', '대해', '말씀', '해주시', '는분들은', '어느정도', '저도', '말씀', '드릴', '있지만', '그게', '아닌', '경우', '에는', '말씀', '드리기가', '애매', '한부분', '양해부탁', '드립니다']\n",
      "Row 23:\n",
      "L Tokenized:  ['아이', '오딘', '야이오다이드도', '추천', '해주세요']\n",
      "Row 24:\n",
      "L Tokenized:  ['상품추천', '광고', '오해', '소지', '있어', '양해부탁', '드립니다']\n",
      "Row 25:\n",
      "L Tokenized:  ['약사님', '좋은', '정보', '감사', '합니다', '혹시', '베르베린', '부작용', '으로', '백혈구', '수치', '감소', '있는', '지요']\n",
      "Row 26:\n",
      "L Tokenized:  ['부작용', '중에', '한가지', '이기는', '한데', '이건', '정말', '드문케이스', '입니다', '면역', '반응의', '이상', '있는', '경우', '베르베린', '으로', '인해', '비정상', '적으로', '감소', '했다는', '얘기', '있습니다']\n",
      "Row 27:\n",
      "L Tokenized:  ['2개', '월전에', '만성염증', '없애준다', '영상', '에서', '우벤자임', '대해', '알게', '되었고', '우벤자임', '계속', '품절', '인해', '대체', '품으로', '메가자', '이란걸', '알게', '되었습니다', '주요', '성분', '우벤자임', '비슷', '하게', '들어', '가있는데', '대체', '품으로', '메가자', '임을', '복용', '해도', '될런지', '또한', '위산과다', '증이', '있는', '사람', '복용', '해도', '될런지', '궁금', '하네요']\n",
      "Row 28:\n",
      "L Tokenized:  ['성분', '함량이', '비슷', '하다면', '섭취', '하셔도', '되지만', '저라면', '기다려', '서라도', '메인을', '복용', '같네요', '그리고', '위산과다', '있어', '섭취', '가능', '합니다']\n",
      "Row 29:\n",
      "L Tokenized:  ['베르베린', '시켯는데여', '여성호르몬', '을건드리는', '성분', '잇나여자궁근종수술받아서여', '또생길까바', '석류', '자몽', '이런류가', '잇나여']\n",
      "Row 30:\n",
      "L Tokenized:  ['여성호르몬', '대한', '부분', '아직', '명확', '하게', '밝혀', '없습니다']\n",
      "Row 31:\n",
      "L Tokenized:  ['감사', '합니다', '약사님', '영상', '보고', '오마비', '구매', '했습니다', '제가', '혈압이', '높은데', '오마비', '여기서', '추천', '해주신거', '하나', '추가', '해서', '먹을까', '하는데', '어떨까요']\n",
      "Row 32:\n",
      "L Tokenized:  ['항혈전제', '섭취', '하지', '않으시면', '괜찮다고', '생각', '합니다']\n",
      "Row 33:\n",
      "L Tokenized:  ['11', '감사', '합니답']\n",
      "Row 34:\n",
      "L Tokenized:  ['나우푸드', '베르베닌', '섭취', '하고', '있는데', '이것', '괜찮', '은가요', '400', '이고다른', '성분', '들어', '있는듯', '해요']\n",
      "Row 35:\n",
      "L Tokenized:  ['특정', '제품', '대한', '언급', '광고', '오해', '소지', '있어', '양해부탁', '드립니다']\n",
      "Row 36:\n",
      "L Tokenized:  ['11', '이제품', '나쁘지는', '않은', '거죠']\n",
      "Row 37:\n",
      "L Tokenized:  ['약사님', '제가', '식단', '하고있어', '음식', '귀리', '닭가슴살', '호두', '탄단지', '맞춰먹고', '있어서', '귀리', '하루', '300', '섭취', '하는데요', '귀리', '300', '아연', '12', '들어', '있고', '제가', '먹는', '라익', '투퍼데이', '25', '닭가슴살', '34', '있어', '하루', '아연', '섭취', '량이', '40', '넘어가는데', '종합비타민', '바꿔야', '까요', '아니면', '자연식', '영양제', '비해', '흡수', '안되어', '괜찮', '을까요']\n",
      "Row 38:\n",
      "L Tokenized:  ['바꾸시는게', '좋다고', '봅니']\n",
      "Row 39:\n",
      "L Tokenized:  ['아침공복', '아니면', '식후', '언제', '먹는게', '좋을까요', '공복', '먹는게', '좋다', '아침', '유산균', '보울라디', '글루타치온', '알파리포산', '섭취', '중인데', '같이', '섭취', '해도', '괜찮', '을까요']\n",
      "Row 40:\n",
      "L Tokenized:  ['점심', '공복', '이나', '저녁', '공복', '섭취', '하시면', '좋을', '합니다']\n",
      "Row 41:\n",
      "L Tokenized:  ['11', '댓글', '너무', '감사', '합니다']\n",
      "Row 42:\n",
      "L Tokenized:  ['구매', '시기인데', '고맙습니다']\n",
      "Row 43:\n",
      "L Tokenized:  ['도움', '되셨다니', '다행', '입니다']\n",
      "Row 44:\n",
      "L Tokenized:  ['이노시톨', '복용중', '입니다', '이것', '혈당', '작용', '하는걸까요', '같이복용', '해도될까요']\n",
      "Row 45:\n",
      "L Tokenized:  ['보조적', '으로', '활용', '가능', '하기는', '합니다']\n",
      "Row 46:\n",
      "L Tokenized:  ['약사님', '땡큐', '베리마취', '다른', '모르겠', '변은', '나와요']\n",
      "Row 47:\n",
      "L Tokenized:  ['도움', '되셨다니', '다행', '입니다']\n",
      "Row 48:\n",
      "L Tokenized:  ['알약', '못먹어서', '물에', '녹여', '먹어도', '됩니', '혹시', '이런', '종류', '있는', '지요']\n",
      "Row 49:\n",
      "L Tokenized:  ['섭취', '하셔도', '괜찮', '습니다']\n",
      "Row 50:\n",
      "L Tokenized:  ['당뇨약', '먹고있는사람도', '먹어도', '되나요']\n",
      "Row 51:\n",
      "L Tokenized:  ['혈당', '유지', '된다면', '저용량', '부터', '시작', '하셔도', '괜찮', '습니다']\n",
      "Row 52:\n",
      "L Tokenized:  ['고혈압', '에도', '효과', '있나요']\n",
      "Row 53:\n",
      "L Tokenized:  ['보조적', '으로', '활용', '가능', '합니다']\n",
      "Row 54:\n",
      "L Tokenized:  ['안녕', '하세요잘보고', '있습니다', '요즘', '센트륨', '구미', '를먹고있습니다', '아이들', '이잘먹서할수없이', '먹습니다', '구미', '영양제', '어떨지', '오메가3', '가동시에', '들어', '있는걸로먹고있는데', '구미제품들', '한번', '영상', '에다뤄주심', '감사', '하겠습니다']\n",
      "Row 55:\n",
      "L Tokenized:  ['기호', '되면', '내용', '참고', '하도록', '하겠습니다']\n",
      "Row 56:\n",
      "L Tokenized:  ['우연', '내츄럴팩터', '베르베린', '리포미셀', '구해서', '먹고있는데', '스완슨', '어드밴스드', '제품', '셋중', '추천', '하시는지', '그냥', '셋중', '암거나', '먹어도', '괜찮', '을까요', '이왕', '이면', '하는', '마음', '여쭤봅니다']\n",
      "Row 57:\n",
      "L Tokenized:  ['이건', '개인', '선택', '이라', '제가', '이거', '다라고', '말씀', '드리기가', '어렵습니다']\n",
      "Row 58:\n",
      "L Tokenized:  ['약사님', '류마티스', '고지혈', '고혈압', '부정맥', '디스크', '신경증약', '퇴행성관절염', '복용', '중인데', '먹어도', '되나요']\n",
      "Row 59:\n",
      "L Tokenized:  ['항혈전제', '섭취', '하고', '계시는', '아니라면', '크게', '문제', '없을', '합니다']\n",
      "Row 60:\n",
      "L Tokenized:  ['저는', '좋게', '솔라레이', '베르베린', '품절', '전에', '구해서', '먹고', '있네요', '이거', '먹은', '후에', '장이', '좋아진듯', '느낌', '들어', '한가지', '질문', '베르베린', '공복', '먹는게', '좋나요', '아님', '식후에', '먹는게', '좋나요']\n",
      "Row 61:\n",
      "L Tokenized:  ['공복', '섭취', '하시는게', '흡수', '율에서는', '낫다고', '생각', '합니다']\n",
      "Row 62:\n",
      "L Tokenized:  ['비타민', '127', '같이', '섭취', '해도', '될까요']\n",
      "Row 63:\n",
      "L Tokenized:  ['괜찮', '습니다']\n",
      "Row 64:\n",
      "L Tokenized:  ['베르베린', '부작용', '장내', '세균', '다죽인다는데', '사실', '인가요', '단독', '으로', '장기간', '복용', '해도', '되는건', '가요']\n",
      "Row 65:\n",
      "L Tokenized:  ['저용량', '으로', '섭취', '하시는', '괜찮다고', '생각', '합니다']\n",
      "Row 66:\n",
      "L Tokenized:  ['안녕', '하세요', '약사님', '보며', '부모님', '저를', '위해', '열심히', '영양제', '공부', '중인', '사람', '입니다', '베르베린', '구매', '했는데', '과민성', '대장', '증후군', '사람', '에게도', '효과', '있다고', '하셨는데', '프로바이오틱', '이랑', '같이', '먹어도', '상관', '없겠죠', '유산균', '약사님', '추천', '해주신', '중에', '노란색', '박스로', '100', '먹고', '설사', '변비', '없어졌습니다', '아침', '빈속', '프로바이오틱', '저녁', '베르베린', '나을까요']\n",
      "Row 67:\n",
      "L Tokenized:  ['베르베린', '오전', '드시면', '유산균', '저녁', '섭취', '하심을', '권장', '드립니다']\n",
      "Row 68:\n",
      "L Tokenized:  ['안녕', '하세요', '궁금', '한게', '있는데요', '성분', '표에', '133', '표기', '되어', '있는', '것을', '변환', '하면', '어떻게', '될까요']\n",
      "Row 69:\n",
      "L Tokenized:  ['이건', '성분', '마다', '다르기는', '한데', '보통', '그렇습니다']\n",
      "Row 70:\n",
      "L Tokenized:  ['당뇨', '인인데요', '베르베른', '복용', '하니', '살이', '빠져서', '중단', '했답니다', '나이', '있어서', '살이', '빠지', '근육', '빠지는것', '같아', '어떻게', '해야']\n",
      "Row 71:\n",
      "L Tokenized:  ['유청단백질', '같은', '성분', '섭취해주시면', '좋을', '같습니다']\n",
      "Row 72:\n",
      "L Tokenized:  ['선생님', '정보', '감사', '합니다', '2월', '부터', '당뇨', '진단으로', '처방', '먹고', '있는데', '병행', '해서', '복용', '해도', '되는건지', '궁금', '합니다']\n",
      "Row 73:\n",
      "L Tokenized:  ['혈당', '유지', '된다면', '섭취', '하셔도', '괜찮', '습니다']\n",
      "Row 74:\n",
      "L Tokenized:  ['작년', '유방암', '수술', '하고', '방사선', '항암', '끝나', '6개월', '지났습니다', '혹시', '베르베린', '복용', '해도', '될까요']\n",
      "Row 75:\n",
      "L Tokenized:  ['치료', '끝나', '셨거나', '복용', '안하신다면', '크게', '문제', '없을', '같습니다']\n",
      "Row 76:\n",
      "L Tokenized:  ['내추럴', '팩터스', '리포미셀', '저거', '샀었는데', '약사님', '영상', '나오고서', '업자', '가격', '올리', '배송', '안해주드라고요그러믄서', '취소', '하라고', '막결국엔', '취소하고', '같은', '가격에', '캐나다', '산으로', '구했어여아우']\n",
      "Row 77:\n",
      "L Tokenized:  ['그렇군요', '안타깝습니다']\n",
      "Row 78:\n",
      "L Tokenized:  ['약사님', '네츄럴', '팩터스는', '복용', '어떻게', '해야하나요']\n",
      "Row 79:\n",
      "L Tokenized:  ['공복', '섭취', '하시면', '됩니']\n",
      "Row 80:\n",
      "L Tokenized:  ['전립선', '좋은약', '뭐가있을까요']\n",
      "Row 81:\n",
      "L Tokenized:  ['안녕', '하세요', '양질의', '아연', '호박씨', '추출물', '라이코펜', '기본', '베이스', '조합', '해주세요', '베타시스테롤', '전립선', '염에', '도움', '있습니다']\n",
      "Row 82:\n",
      "L Tokenized:  ['11', '베타시스테롤', '구입', '해서먹어보겠습니다', '감사', '합니다']\n",
      "Row 83:\n",
      "L Tokenized:  ['혹시', '알러지', '생기', '언제', '쯤다시', '복용', '해도', '좋을까요']\n",
      "Row 84:\n",
      "L Tokenized:  ['보통', '2주', '정도', '끊었다', '다시', '섭취', '시에', '똑같이', '알러지', '생기', '신다면', '중단', '하셔야', '합니다']\n",
      "Row 85:\n",
      "L Tokenized:  ['당뇨약', '메타', '포민을', '1500', '밀리', '먹고있는데', '베르베린', '같이', '먹어도', '괜찮', '나요']\n",
      "Row 86:\n",
      "L Tokenized:  ['혈당', '유지', '되고', '있는', '경우라면', '저용량', '부터', '섭취', '하세요']\n",
      "Row 87:\n",
      "L Tokenized:  ['식전', '식후', '언제', '복용', '하면', '효과', '좋은', '가요']\n",
      "Row 88:\n",
      "L Tokenized:  ['공복', '섭취', '하심을', '권장', '드립니다']\n",
      "Row 89:\n",
      "L Tokenized:  ['애사비', '먹고있는데', '애사비', '식전', '베르베린', '식전', '인데', '같이', '괜찮', '을까요']\n",
      "Row 90:\n",
      "L Tokenized:  ['같이', '섭취', '하시는', '권장', '드리지는', '않습니다']\n",
      "Row 91:\n",
      "L Tokenized:  ['선생님', '제가', '전린선염이', '계속', '재발', '해서', '고생중', '인데', '전립선', '염에', '좋다', '영양제', '챙겨먹으려고', '하는데', '그중', '에서', '오메가3지방산', '전립선', '염에', '도움', '된다', '는데', '맞을까요', '적정량', '먹으면', '염증', '개선에', '좋다', '말들', '있고', '전립선', '암을', '유발', '한다는', '말들', '있고해서', '너무', '고민', '됩니다', '그리', '라이코펜', '영양제', '도움', '이될까요']\n",
      "Row 92:\n",
      "L Tokenized:  ['안녕', '하세요', '양질의', '아연', '호박씨', '추출물', '라이코펜', '기본', '베이스', '조합', '해주세요', '베타시스테롤', '전립선', '염에', '도움', '있습니다']\n",
      "Row 93:\n",
      "L Tokenized:  ['11', '답글', '너무', '너무', '감사', '합니다', '호박씨', '추출물', '직구', '주문', '했고', '라이코펜', '영양제', '나우푸드', '유명', '한걸로', '아는데', '현재', '쿠팡에서', '직구', '조금', '어려워서', '다른', '브랜드', '추천', '해주실수', '있나요']\n",
      "Row 94:\n",
      "L Tokenized:  ['고약사님', '단추', '있는', '어디', '사셨어요', '저도', '한번', '입어', '미리', '감사', '합니다']\n",
      "Row 95:\n",
      "L Tokenized:  ['말씀', '드리기', '어려운', '양해부탁', '드립니다']\n",
      "Row 96:\n",
      "L Tokenized:  ['변비', '심한', '사람', '인데', '베르베린', '먹으면', '장내', '유익', '균이', '사라지는', '아닐까', '걱정', '입니다', '혈당', '때문에', '베르베린', '복용', '하곤', '싶은데', '어떻게', '하는', '좋을까요']\n",
      "Row 97:\n",
      "L Tokenized:  ['베르베린', '오전', '공복', '섭취', '하시고', '저녁', '공복', '유산균', '섭취', '하시면', '됩니']\n",
      "Row 98:\n",
      "L Tokenized:  ['약은', '쿠팡', '이상', '안삽니다', '물건', '달랐어요', '저렴', '하기에', '쿠팡', '선택', '하시는거면', '댓글', '보시고', '사세요', '이제', '찝찝', '해서', '그냥', '아이허브', '에서', '삽니다']\n",
      "Row 99:\n",
      "L Tokenized:  ['후기', '남겨주셔', '감사', '합니다']\n",
      "Row 100:\n",
      "L Tokenized:  ['셔츠', '좌표', '부탁', '해요']\n",
      "Row 101:\n",
      "L Tokenized:  ['좌표', '넣어', '드릴', '없는', '양해부탁', '드립니다']\n"
     ]
    }
   ],
   "source": [
    "# 각 행에 대해 l_tokenized와 maxscore_tokenized 결과를 함께 출력\n",
    "for index, row in data.iterrows():\n",
    "    print(f\"Row {index}:\")\n",
    "    print(\"L Tokenized: \", row['tokenized'])\n",
    "    # print(\"MaxScore Tokenized: \", row['maxscore_tokenized'])\n",
    "    # print(\"\\n\")\n",
    "    if index > 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "channelId\n",
       "UC3iSLVH0MxHfwO69oHKpvog    [리틀약사, 최고이십니다, 감사, 합니다, 비문증, 브로멜라인, 파파인에, 대해서,...\n",
       "UC6ggXTuBVchhwHeQ12Ita1w    [베르베린, 복용, 방법, 안내, 복용, 이미, 아시겠지만, 점진적으로, 증량, 하...\n",
       "UCCMFTDGarjgZLc1DlIbbvRg    [잘보고, 갑니다, 감사, 합니다, 사람, 소화, 시켜, 열량을, 뽑아, 없는, 셀...\n",
       "UCMFk5S7g5DY-CZNVh_Kyz_A    [야채, 많이, 드시고, 가끔, 요구르트, 챙겨, 드시면, 어느정도, 예방, 제일,...\n",
       "UCY-mXLM6DsS9cmSwlh0tqSA    [담백하루, 아르기닌, 변비, 유산균, 공구, 오픈, 구매, 링크, 트랜짓, 유산균...\n",
       "Name: tokenized, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 채널별로 데이터 그룹화 및 토큰 리스트 합치기\n",
    "grouped_data = data.groupby('channelId')['tokenized'].agg(lambda x: [token for sublist in x for token in sublist])\n",
    "grouped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 채널별로 상위 20개 단어 추출\n",
    "top_words_per_channel = {}\n",
    "for channel_id, tokens in grouped_data.items():\n",
    "    # 토큰의 빈도수 계산\n",
    "    token_counts = Counter(tokens)\n",
    "    # 가장 빈번한 30개 단어 추출\n",
    "    top_words = token_counts.most_common(30)\n",
    "    top_words_per_channel[channel_id] = top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel UC3iSLVH0MxHfwO69oHKpvog:\n",
      "합니다: 1531\n",
      "감사: 1276\n",
      "으로: 830\n",
      "상담: 615\n",
      "마그네슘: 553\n",
      "비타민: 548\n",
      "먹고: 543\n",
      "하고: 541\n",
      "리틀약사: 525\n",
      "영양제: 517\n",
      "밴드: 508\n",
      "제품: 488\n",
      "복용: 465\n",
      "영상: 463\n",
      "많이: 426\n",
      "약사님: 398\n",
      "네이버: 398\n",
      "좋은: 378\n",
      "83355090: 372\n",
      "오메가3: 367\n",
      "너무: 365\n",
      "입니다: 365\n",
      "하시면: 354\n",
      "건강: 353\n",
      "보다: 350\n",
      "해서: 346\n",
      "하는: 346\n",
      "섭취: 342\n",
      "먹어도: 341\n",
      "도움: 340\n",
      "\n",
      "\n",
      "Channel UC6ggXTuBVchhwHeQ12Ita1w:\n",
      "합니다: 749\n",
      "감사: 676\n",
      "복용: 631\n",
      "약사님: 485\n",
      "영상: 312\n",
      "으로: 273\n",
      "섭취: 251\n",
      "좋은: 251\n",
      "영양제: 230\n",
      "하고: 224\n",
      "효과: 216\n",
      "제품: 215\n",
      "있습니다: 212\n",
      "에서: 203\n",
      "정보: 193\n",
      "하세요: 177\n",
      "선생님: 176\n",
      "입니다: 168\n",
      "하는: 165\n",
      "많이: 165\n",
      "너무: 163\n",
      "건강: 160\n",
      "저는: 160\n",
      "도움: 158\n",
      "멜라토: 158\n",
      "먹고: 154\n",
      "레스베라트롤: 153\n",
      "베르베린: 151\n",
      "피세틴: 151\n",
      "안녕: 145\n",
      "\n",
      "\n",
      "Channel UCCMFTDGarjgZLc1DlIbbvRg:\n",
      "감사: 84\n",
      "합니다: 72\n",
      "약사님: 36\n",
      "이지약사님: 33\n",
      "효과: 32\n",
      "너무: 30\n",
      "좋은: 29\n",
      "제품: 24\n",
      "멜라토: 24\n",
      "저도: 23\n",
      "같이: 22\n",
      "바르고: 20\n",
      "피부: 19\n",
      "입니다: 19\n",
      "영상: 19\n",
      "정보: 18\n",
      "해요: 18\n",
      "크림: 17\n",
      "으로: 15\n",
      "저는: 14\n",
      "에서: 14\n",
      "있는데: 14\n",
      "하는: 13\n",
      "이지: 13\n",
      "처방: 13\n",
      "오랜만: 13\n",
      "하세요: 13\n",
      "있어요: 13\n",
      "멜라노사: 13\n",
      "궁금: 12\n",
      "\n",
      "\n",
      "Channel UCMFk5S7g5DY-CZNVh_Kyz_A:\n",
      "합니다: 10553\n",
      "감사: 8302\n",
      "섭취: 5436\n",
      "영양제: 3379\n",
      "비타민: 3110\n",
      "약사님: 3055\n",
      "으로: 3051\n",
      "제품: 2882\n",
      "오메가3: 2878\n",
      "복용: 2815\n",
      "영상: 2587\n",
      "드립니다: 2579\n",
      "괜찮: 2561\n",
      "해주셔서: 2526\n",
      "도움: 2432\n",
      "시청: 2357\n",
      "너무: 2339\n",
      "있습니다: 2218\n",
      "입니다: 2104\n",
      "추천: 1873\n",
      "먹고: 1813\n",
      "생각: 1781\n",
      "좋은: 1763\n",
      "해서: 1626\n",
      "마그네슘: 1620\n",
      "에서: 1612\n",
      "11: 1611\n",
      "있는: 1593\n",
      "말씀: 1582\n",
      "같이: 1557\n",
      "\n",
      "\n",
      "Channel UCY-mXLM6DsS9cmSwlh0tqSA:\n",
      "1등: 1560\n",
      "합니다: 1483\n",
      "감사: 1381\n",
      "추천: 1353\n",
      "영양제: 1330\n",
      "양과자: 928\n",
      "제품: 922\n",
      "비타민: 770\n",
      "도움: 605\n",
      "으로: 551\n",
      "영상: 539\n",
      "있습니다: 529\n",
      "구매: 514\n",
      "유산균: 505\n",
      "너무: 498\n",
      "10: 442\n",
      "해서: 414\n",
      "좋은: 404\n",
      "입니다: 393\n",
      "먹고: 380\n",
      "오메가3: 368\n",
      "복용: 366\n",
      "있는: 349\n",
      "드립니다: 346\n",
      "에서: 338\n",
      "많이: 330\n",
      "먹어도: 326\n",
      "괜찮: 326\n",
      "섭취: 322\n",
      "하세요: 321\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "for channel_id, words in top_words_per_channel.items():\n",
    "    print(f\"Channel {channel_id}:\")\n",
    "    for word, count in words:\n",
    "        print(f\"{word}: {count}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
